<a name="AMytj"></a>
# 文件
- 源文件：[漫谈线上问题排查.pdf](https://huangduoyu.yuque.com/attachments/yuque/0/2021/pdf/1461694/1634210922927-8789e3cb-61b8-4de0-881b-a0b3bfbab75f.pdf?_lake_card=%7B%22src%22%3A%22https%3A%2F%2Fhuangduoyu.yuque.com%2Fattachments%2Fyuque%2F0%2F2021%2Fpdf%2F1461694%2F1634210922927-8789e3cb-61b8-4de0-881b-a0b3bfbab75f.pdf%22%2C%22name%22%3A%22%E6%BC%AB%E8%B0%88%E7%BA%BF%E4%B8%8A%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5.pdf%22%2C%22size%22%3A8862824%2C%22type%22%3A%22application%2Fpdf%22%2C%22ext%22%3A%22pdf%22%2C%22status%22%3A%22done%22%2C%22taskId%22%3A%22u89c2ca82-4aa7-4722-8d63-91ace9dfe03%22%2C%22taskType%22%3A%22upload%22%2C%22id%22%3A%22u7f917033%22%2C%22card%22%3A%22file%22%7D)
- 云地址[https://www.aliyundrive.com/s/87WPrULGTgp](https://www.aliyundrive.com/s/87WPrULGTgp)
<a name="nwaoV"></a>
# 章节
<a name="ObHJz"></a>
## 第一章 线上问题排查的方法论
<a name="Ud8mq"></a>
### 1.线上问题处理流程及开发需关注的关键点
![image.png](https://cdn.nlark.com/yuque/0/2021/png/1461694/1626935328666-841bb1b8-5cdf-4bba-86aa-7bbc29e50c13.png#clientId=uf0006556-435b-4&from=paste&height=173&id=u9b282f80&margin=%5Bobject%20Object%5D&name=image.png&originHeight=346&originWidth=859&originalType=binary&ratio=1&size=168722&status=done&style=none&taskId=ud1188aa4-81fe-4517-bce8-9af160418e0&width=429.5)<br />主要关注这三个点：**发现**，**恢复**，**分析**，下面会谈谈常用技巧。
<a name="NqihT"></a>
#### 1.故障发现

   - 对系统架构的了解，知道上下游系统的依赖，中间件的依赖等。
   - 对业务的了解，主要是逻辑上的错误，还是编码错误，能预判定位。
   - 依赖监控体系，包括CPU，IO，内存，线程，JVM，日志，中间件监控，预警。
<a name="TNS8m"></a>
#### 
<a name="W14dp"></a>
#### 2.故障恢复

   - 处理故障的原则：**及时恢复，减少对业务影响**。
   - 恢复的常见手段：版本回退，紧急修复，服务降级，摘机，重启等，根据场景选择最优的方式。

<a name="bcxoh"></a>
#### 3.故障分析

   - 注意保留现场，Java常见保留现场的6种方法是：（围绕，内存，线程，对象去保留）
         1. 执行 jstack stack.log，每隔几秒钟保存一次堆栈信息执行 
         1. jstat -gcutil ，查看堆内存各区域的使用率以及GC情况
         1. 执行 jmap -histo | head -n20，查看堆内存中的存活对象，并按空间排序
         1. 执行 jmap -dump:format=b,file=heap ，dump堆内存文件，dump之前可以从做摘机处理，因为dump过程中会发生FGC，引发STW
         1. 执行 top 命令，shift + p 可以按 CPU 使用率倒排，记录最消耗资源的进程信息
         1. 执行 free -m 命令，shift + m 可以按照内存使用量倒排，记录最好资源的进程信息
   - 常见Linux命令行和工具
         1. 磁盘：df -h网络：netstat -nat | awk '{print $6}' | sort | uniq -c | sort -rn
         1. IO：iostat
         1. 内存：free -m
         1. CPU：top -Hp
         1. JVM：jstack（线程）、jmap（内存）、jstat（GC）
         1. 其他工具：JVisual、MAT、arthas
<a name="f3PR8"></a>
### 2.常见的系统监控和选型建议
<a name="xfgmD"></a>
#### 1.常见系统监控选型

- ![image.png](https://cdn.nlark.com/yuque/0/2021/png/1461694/1626938320668-b51b7cfb-dc93-49e3-82db-77ca4c9ad970.png#clientId=uf0006556-435b-4&from=paste&height=209&id=u9e840cf5&margin=%5Bobject%20Object%5D&name=image.png&originHeight=418&originWidth=838&originalType=binary&ratio=1&size=172264&status=done&style=none&taskId=u8297d5d5-d022-4420-a85b-0379cd8a61b&width=419)
- Zabbix（老牌产品成熟，采集方式丰富，产品成熟，配置管理方便；性能有瓶颈，二次开发难度大）
- OpenFalcon(小米出品，流行，自动采集能力，存储能力强，个性化监控支持；安装复杂，UI差，发展差）
- Prometheus(普罗米修斯,新型监控系统，轻便管理,数据模型灵活，查询语句强，支持云环境；功能不完善）

<a name="OTSz8"></a>
#### 2.建议
根据需求，选择最合适的健康系统，不要盲目追求流行技术。

<a name="iZAmO"></a>
## 第二章 典型案例的排查分析过程
<a name="ab65j"></a>
### 1.多线程死锁问题排查案例
<a name="yakBW"></a>
#### 过程

1. 收到告警，先看服务各纬度的数据，服务调用量，超时量，错误日志，JVM监控等。
1. 排查服务中间件和存储节点，发现有大量Mysql的慢查询。
1. 分析慢查询原因，紧急优化慢查询。
1. 问题依旧没没解决，发现某个线程池中依旧存在大量未消费的任务，然后保留一台故障机器，做好故障现场备份，重启其中一台服务解决问题。
1. 到这一步，以为完事了。。。。可坑还在后面。。。。
1. 过了很长时间，把第4步的故障机器，重新加入到节点，发现线程池中的任务依旧未消费掉，然后找到进程号： ps aux | grep "adclick"，找到Dump快照分析  jstack pid > /tmp/stack.txt，发现线程池中的线程都处于等待状态，锁住了。
1. 找到业务代码，发现是父子任务都用同一个线程池，当一个线程池中全部是父任务，且子任务未执行完的情况下，所有父任务都锁死了。

<a name="niQDh"></a>
#### 解决方案
父子任务隔开，不要用同一个线程池。<br />还有个未解决的问题，他重启服务怎么保证数据不丢失呢？ 怎么处理这些异常数据呢？

<a name="NpchI"></a>
#### 总结

- 使用固定线程数的线程池存在OOM风险，在阿里巴巴 Java 开发手册中也明确指出，而且用的词是『不允许』使用 Executors 创建线程池。 而是通过ThreadPoolExecutor 去创建，这样让写的同学能更加明确线程池的运行规则和核心参数设置，规避资源耗尽的风险。
- 异步过程，通过线程池或者 MQ 来实现异步化处理都是可选的方案。

<a name="M24ya"></a>
### 2.RPC超时问题排查案例
<a name="wCS91"></a>
#### 1.场景描述
![image.png](https://cdn.nlark.com/yuque/0/2021/png/1461694/1626944947809-e5bc62f5-e2ce-4ca6-b282-f9b6afd530ef.png#clientId=uf0006556-435b-4&from=paste&height=224&id=ue76b005e&margin=%5Bobject%20Object%5D&name=image.png&originHeight=447&originWidth=839&originalType=binary&ratio=1&size=167542&status=done&style=none&taskId=u0ca4cbe2-be69-44c6-b61b-3ab162ddfd8&width=419.5)
<a name="wqnR5"></a>
#### 2.原因分析

   - 根本问题是推荐服务，因为推荐服务依赖的redis集群不可用导致了超时，进而导致线程池耗尽。
   - 错误的设置了超时时间和重试次数，导致最终调用端出现了异常，还导致降级失败。

![image.png](https://cdn.nlark.com/yuque/0/2021/png/1461694/1626944956996-d0a7bbea-59f2-4256-a8c4-95c30d0abef8.png#clientId=uf0006556-435b-4&from=paste&height=141&id=u5dfe76d7&margin=%5Bobject%20Object%5D&name=image.png&originHeight=282&originWidth=834&originalType=binary&ratio=1&size=182611&status=done&style=none&taskId=udc69b055-b236-44ea-a15c-9d0510bac0d&width=417)
<a name="nilXV"></a>
#### 3.解决方案
重新设置超时时间和重试次数

<a name="yVZh9"></a>
#### 4.总结

- **设置超时时间的好处？**
   1. consumer请求provider，如不设置超市时间，则consumer的总响应时间会过长，如果调用链路过长，会导致接口变慢，阻塞，及影响服务的负载情况。
   1. 设置超时阀值，可以避免非核心功能的调用，而阻断主流程，如商品服务依赖评论服务，评论服务挂了，不影响商品的正常展示。
   1. 设置重试次数，可避免临时阻塞，或者网络抖动等情况引起的异常调用，但是接口要保证好幂等性。

- **设置超时时间需要考虑的几点**
   1. 先了解依赖服务的响应时间是多少，在调用方超时的时间基础上增加50%。
   1. 最外层的超时时间，要大于里面最长的耗时时间。（如果 RPC 框架支持多粒度的超时设置，则：全局超时时间应该要略大于接口级别最长的耗时时间，每个接口的超时时间应该要略大于方法级别最长的耗时时间，每个方法的超时时间应该要略大于实际的方法执行时间）
   1. 区分是否可重试，要保证操作的幂等性。
   1. 尽量在服务端做超时设置，避免下游客户端不设置，出现隐患。
   1. 如果调用方是高QPS服务，则必须考虑服务方超时情况下的降级和熔断策略。（比如超过10%的请求出错，则停止重试机制直接熔断，改成调用其他服务、异步MQ机制、或者使用调用方的缓存数据）

<a name="oUB2s"></a>
### 3.FGC,YGC问题排查案例
<a name="V3X9P"></a>
#### 前提条件

- 先要搞清楚这三点的原理：JVM的内存结构，GC回收机制，GC回收算法
- FGC：老年代垃圾回收异常，JVM卡顿，正常耗时不超过3秒，频率几个小时，甚至一天才回收一次，高并发的服务要特别注意。
- YGC：新生代垃圾回收异常，JVM卡顿，正常耗时不超过1秒，频率是比较高的，但是过于频繁和耗时，也会影响服务性能。
- 围绕“耗时”，“频次”，“空间”，三个维度来分析GC情况。

<a name="Jq65E"></a>
#### 1.通常有哪些原因导致FGC？

1. 大对象：系统一次性加载了过多数据到内存中（比如SQL查询未做分页），导致大对象进入了老年代。
1. 内存泄漏：频繁创建了大量对象，但是无法被回收（比如IO对象使用完后未调用close方法释放资源），先引发FGC，最后导致OOM.
1. 频繁创建生命周期长的对象，对象的存活年龄超过分代年龄时会进入老年代,引发FGC.(即本文中的案例)
1. 程序BUG导致动态生成了很多新类，使得 Metaspace 不断被占用，先引发FGC，最后导致OOM.
1. 代码中显式调用了gc方法，包括自己的代码甚至框架中的代码。
1. JVM参数设置问题：包括总内存大小、新生代和老年代的大小、Eden区和S区的大小、元空间大小、垃圾回收算法等等。

<a name="qP7Ct"></a>
#### 2.排查GC问题时通常用哪些工具？

- 公司的监控系统：大部分公司都会有，可全方位监控JVM的各项指标。2、JDK的自带工具，包括jmap
- jstat等常用命令：
   1. 查看堆内存各区域的使用率以及GC情况：jstat -gcutil -h20 pid 1000
   1. 查看堆内存中的存活对象，并按空间排序：jmap -histo pid | head -n20
   1. dump堆内存文件： jmap -dump:format=b,file=heap pid
- 可视化的堆内存分析工具：JVisualVM、MAT等。

<a name="eqx48"></a>
#### 3.问题排查指南

1. 查看监控，了解出现问题发生的时间点，GC频率等，以及最近有无发版记录，基础组件升级等，
1. 了解JVM的参数（堆空间大小设置，什么收集器，参数是否合理等）
1. 针对1种的可能原因，逐一排除，如：元空间打满，内存泄露，显示调用GC函数等。
1. 查看Dump日志，查看大对象，找出可疑对象，定位到代码层面，结合设置的JVM参数分析问题。

<a name="QkMXx"></a>
### 4.数据库中间件Bug排查案例
<a name="a07pe"></a>
#### 1.思路

1. 排查版本依赖，复现问题，JDBC依赖更换，等其他依赖版本更换，逐一排查，确定问题发生的范围。
1. 排查数据库，直接通过Demo用JDBC连接Mysql,无法复现问题。<br />排查配置ShardingProxy的参数，逐个替换，排查可能发生的问题，发现把Url上面两个关于预编译的参数设置给去掉，问题就解决了。
1. 排查SQL传输过程，通过Wireshark抓包，分析Mysql协议，在gitHub上提问，官方不接受这是个bug。
1. 官方开发者，收到问题，和证据，提交了代码，解决了问题。
1. 自己结合源码分析，问题出现的原因。

<a name="rYEIP"></a>
#### 2.总结
开源组件也有坑，官方开源大牛也是人，要敢于挑战！要有钻研精神，探索精神。<br />知其然知其所以然！

<a name="z9m7I"></a>
# 题外话
有理有据，主要学习一下解决问题的思路和方法，细节方面自己去琢磨。<br />	
